{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import get_prices as hist\n",
    "import tensorflow as tf\n",
    "from preprocessing import DataProcessing as process\n",
    "import pandas_datareader.data as pdr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rc(\"font\", size=14)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "palu=pd.read_excel('C:\\\\Users\\\\ASUS\\\\Documents\\\\these\\\\code_these-master\\\\palu_impute001.xlsx')\n",
    "paluu=pd.DataFrame(palu, columns = ['PPOIDS', 'TEMPERATURE', 'S_M8_APPETIT', 'S_FATIGUE', 'S_ARTHRALGI',\n",
    "                                    'S_T_DIGESTIF', 'S_VERTIGE', 'S_FRISSON', 'S_MYALGIE', 'S_DABDO', \n",
    "                                    'S_VOMISS', 'S_NAUSEE', 'S_CEPHALE', 'S_FIEVRE','TDR'])\n",
    "\n",
    "# Nous séparons notre variable cible en deux parties. Une partie pour entrainer y_train et une partie pour tester y_test\n",
    "X = paluu\n",
    "y = palu['Diagnostic']\n",
    "X1_train, MX_test, y1_train, My_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "#Suréchantillonnage avec l'algorithme SMOTE\n",
    "X1 = paluu\n",
    "y1 =palu['Diagnostic']\n",
    "y1= pd.DataFrame(y1)\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "Re= SMOTE(random_state=0)\n",
    "#X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.3, random_state=0)\n",
    "#columns = X1_train.columns\n",
    "Re_data_X1,Re_data_y1=Re.fit_sample(X1_train, y1_train)\n",
    "Re_data_X1 = pd.DataFrame(data=Re_data_X1 )\n",
    "\n",
    "\n",
    "# Nous séparons notre jeu données palu en deux parties. Une partie pour entrainer X_train et un partie pour tester X_test\n",
    "# Nous séparons notre variable cible en deux parties. Une partie pour entrainer y_train et une partie pour tester y_test\n",
    "X_train =Re_data_X1\n",
    "y_train=Re_data_y1\n",
    "#y_train= pd.DataFrame(Re_data_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30698, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('pal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9170304536819458\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=15, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    " \n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "#dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = X_train\n",
    "Y = y_train\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10, verbose=0)\n",
    "# evaluate using 10-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.919506 using {'batch_size': 10, 'epochs': 6, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.888201 (0.037416) with: {'batch_size': 5, 'epochs': 2, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.883738 (0.028970) with: {'batch_size': 5, 'epochs': 2, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.909799 (0.056381) with: {'batch_size': 5, 'epochs': 2, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.882370 (0.037213) with: {'batch_size': 5, 'epochs': 2, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.897257 (0.061869) with: {'batch_size': 5, 'epochs': 2, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.915793 (0.061191) with: {'batch_size': 5, 'epochs': 2, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.885856 (0.036036) with: {'batch_size': 5, 'epochs': 4, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.890025 (0.030043) with: {'batch_size': 5, 'epochs': 4, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.909115 (0.050974) with: {'batch_size': 5, 'epochs': 4, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.908072 (0.056731) with: {'batch_size': 5, 'epochs': 4, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.913545 (0.056739) with: {'batch_size': 5, 'epochs': 4, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.915402 (0.063065) with: {'batch_size': 5, 'epochs': 4, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.904489 (0.051737) with: {'batch_size': 5, 'epochs': 6, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.911786 (0.061252) with: {'batch_size': 5, 'epochs': 6, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.911916 (0.066506) with: {'batch_size': 5, 'epochs': 6, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.912633 (0.066524) with: {'batch_size': 5, 'epochs': 6, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.911851 (0.066499) with: {'batch_size': 5, 'epochs': 6, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.915532 (0.063218) with: {'batch_size': 5, 'epochs': 6, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.883510 (0.026191) with: {'batch_size': 10, 'epochs': 2, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.881458 (0.024367) with: {'batch_size': 10, 'epochs': 2, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.901915 (0.048737) with: {'batch_size': 10, 'epochs': 2, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.913512 (0.061067) with: {'batch_size': 10, 'epochs': 2, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.911916 (0.063278) with: {'batch_size': 10, 'epochs': 2, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.917226 (0.053349) with: {'batch_size': 10, 'epochs': 2, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.886638 (0.029943) with: {'batch_size': 10, 'epochs': 4, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.877614 (0.032423) with: {'batch_size': 10, 'epochs': 4, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.912893 (0.062463) with: {'batch_size': 10, 'epochs': 4, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.914099 (0.063117) with: {'batch_size': 10, 'epochs': 4, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.912079 (0.066421) with: {'batch_size': 10, 'epochs': 4, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.912568 (0.061895) with: {'batch_size': 10, 'epochs': 4, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.891654 (0.039833) with: {'batch_size': 10, 'epochs': 6, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.906509 (0.060722) with: {'batch_size': 10, 'epochs': 6, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.912991 (0.062986) with: {'batch_size': 10, 'epochs': 6, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.915434 (0.063139) with: {'batch_size': 10, 'epochs': 6, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.919506 (0.057942) with: {'batch_size': 10, 'epochs': 6, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.912665 (0.064151) with: {'batch_size': 10, 'epochs': 6, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.864975 (0.014998) with: {'batch_size': 20, 'epochs': 2, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.877451 (0.025115) with: {'batch_size': 20, 'epochs': 2, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.885041 (0.027346) with: {'batch_size': 20, 'epochs': 2, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.882500 (0.027018) with: {'batch_size': 20, 'epochs': 2, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.896703 (0.051181) with: {'batch_size': 20, 'epochs': 2, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.888429 (0.039186) with: {'batch_size': 20, 'epochs': 2, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.704118 (0.229740) with: {'batch_size': 20, 'epochs': 4, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.876930 (0.031513) with: {'batch_size': 20, 'epochs': 4, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.890091 (0.037471) with: {'batch_size': 20, 'epochs': 4, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.918724 (0.058803) with: {'batch_size': 20, 'epochs': 4, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.907225 (0.064137) with: {'batch_size': 20, 'epochs': 4, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.915565 (0.063177) with: {'batch_size': 20, 'epochs': 4, 'init': 'uniform', 'optimizer': 'adam'}\n",
      "0.903544 (0.055350) with: {'batch_size': 20, 'epochs': 6, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.816535 (0.099014) with: {'batch_size': 20, 'epochs': 6, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.894260 (0.041806) with: {'batch_size': 20, 'epochs': 6, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.903479 (0.062870) with: {'batch_size': 20, 'epochs': 6, 'init': 'normal', 'optimizer': 'adam'}\n",
      "0.902697 (0.065792) with: {'batch_size': 20, 'epochs': 6, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.919474 (0.055318) with: {'batch_size': 20, 'epochs': 6, 'init': 'uniform', 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy\n",
    " \n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=15, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(15, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    " \n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "#dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = X_train\n",
    "Y = y_train\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# grid search epochs, batch size and optimizer\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "init = ['glorot_uniform', 'normal', 'uniform']\n",
    "epochs = [2, 4, 6]\n",
    "batches = [5, 10, 20]\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
