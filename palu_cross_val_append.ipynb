{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import learning_curve\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rc(\"font\", size=14)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import tensorflow as tf\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import KFold, LeaveOneOut,StratifiedKFold\n",
    "np.random.seed(1000) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du jeu de données\n",
    "palu=pd.read_excel('C:\\\\Users\\\\ASUS\\\\Documents\\\\these\\\\code_these-master\\\\palu_impute001.xlsx')\n",
    "# Chargement du jeu de données\n",
    "palu1=pd.read_excel('C:\\\\Users\\\\ASUS\\\\Documents\\\\these\\\\code_these-master\\\\MORBIDITE.xls')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palu1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection des features du paludisme\n",
    "paluu=pd.DataFrame(palu, columns = [  'TEMPERATURE','S_M8_APPETIT', 'S_FATIGUE', 'S_ARTHRALGI',\n",
    "                                    'S_T_DIGESTIF', 'S_VERTIGE', 'S_FRISSON', 'S_MYALGIE', 'S_DABDO', \n",
    "                                    'S_VOMISS', 'S_NAUSEE', 'S_CEPHALE', 'S_FIEVRE','Diagnostic'])\n",
    "\n",
    "# Nous séparons notre variable cible en deux parties. Une partie pour entrainer y_train et une partie pour tester y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection des features du paludisme\n",
    "paluu1=pd.DataFrame(palu1, columns = [ 'TEMPERATURE', 'S_M8_APPETIT', 'S_FATIGUE', 'S_ARTHRALGI',\n",
    "                                    'S_T_DIGESTIF', 'S_VERTIGE', 'S_FRISSON', 'S_MYALGIE', 'S_DABDO', \n",
    "                                    'S_VOMISS', 'S_NAUSEE', 'S_CEPHALE', 'S_FIEVRE','Diagnostic'])\n",
    "\n",
    "# Nous séparons notre variable cible en deux parties. Une partie pour entrainer y_train et une partie pour tester y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palu2=paluu.append(paluu1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection des features du paludisme\n",
    "X=palu2.iloc[:,0:13]       \n",
    "\n",
    "#X=pd.DataFrame(palu2, columns = [  'TEMPERATURE', 'S_M8_APPETIT', 'S_FATIGUE', 'S_ARTHRALGI',\n",
    "                                    #'S_T_DIGESTIF', 'S_VERTIGE', 'S_FRISSON', 'S_MYALGIE', 'S_DABDO', \n",
    "                                    #'S_VOMISS', 'S_NAUSEE', 'S_CEPHALE', 'S_FIEVRE'])\n",
    "y = palu2['Diagnostic']\n",
    "X1_train, MX_test, y1_train, My_test = train_test_split(X, y,test_size=0.2 ,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas_profiling as pp\n",
    "#pp.ProfileReport(palu2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palu2['Diagnostic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les differentes methodes pour faire une validation croisée(Cross Validation)\n",
    "cv=KFold(5,random_state=0)\n",
    "cv1= LeaveOneOut()\n",
    "cv2=StratifiedKFold(5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Chercher les meilleurs paramètres du modele\n",
    "param_grid={'max_depth': np.arange(12,20), 'criterion':['gini','entropy']}\n",
    "grid=GridSearchCV( DecisionTreeClassifier(),param_grid, cv=cv2)\n",
    "grid.fit(X1_train,y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Afficher le meilleur score\n",
    "print('best score',grid.best_score_)\n",
    "# Afficher les meilleurs parametres\n",
    "print('best score',grid.best_params_)\n",
    "# Recuperer le meilleur modele fourni \n",
    "model=grid.best_estimator_\n",
    "print('best score',model.score(MX_test,My_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=model.predict(MX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b= pd.DataFrame(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.to_csv('predict1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher la matrice de confusion\n",
    "print(confusion_matrix(My_test, model.predict(MX_test)))  \n",
    "# Afficher le rapport de la classification\n",
    "print(classification_report(My_test,model.predict(MX_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracer les courbes d'apprentissages: Veci permet de verifier s'il y a surapprentissage ou apprentissage\n",
    "N, train_score, val_score=learning_curve(model, X1_train, y1_train, train_sizes=np.linspace(0.2, 1.0,5), cv=3)\n",
    "plt.plot(N,val_score.mean(axis=1), label= 'validation')\n",
    "plt.plot(N,train_score.mean(axis=1), label= 'train')\n",
    "plt.xlabel('train_sizes')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "y_pred=model.predict_proba(MX_test)[:,1]\n",
    "# Calculer l'air sous la courbe\n",
    "logit_roc_auc = roc_auc_score(My_test,y_pred)\n",
    "print(logit_roc_auc)\n",
    "# Tracer la courbe roc\n",
    "fpr, tpr, thresholds = roc_curve(My_test, y_pred)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Decision Tree Classifier(area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Courbe ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "param_grid={'max_depth': np.arange(14,20), 'criterion':['gini','entropy'],'n_estimators':[10,50,100, 150, 200,250]}\n",
    "grid3=GridSearchCV( RandomForestClassifier(),param_grid, cv=cv2)\n",
    "grid3.fit(X1_train,y1_train)\n",
    "\n",
    "print('best score',grid3.best_score_)\n",
    "print('best score',grid3.best_params_)\n",
    "model3=grid3.best_estimator_\n",
    "print('best score',model3.score(MX_test,My_test))\n",
    "N, train_score, val_score=learning_curve(model3, X1_train, y1_train, train_sizes=np.linspace(0.2, 1.0,5), cv=cv2)\n",
    "\n",
    "plt.plot(N,val_score.mean(axis=1), label= 'validation')\n",
    "plt.plot(N,train_score.mean(axis=1), label= 'train')\n",
    "plt.xlabel('train_sizes')\n",
    "plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(My_test, model3.predict(MX_test)))  \n",
    "print(classification_report(My_test,model3.predict(MX_test)))\n",
    "\n",
    "\n",
    "y_pred=model3.predict_proba(MX_test)[:,1]\n",
    "logit_roc_auc = roc_auc_score(My_test,y_pred)\n",
    "print(logit_roc_auc)\n",
    "fpr, tpr, thresholds = roc_curve(My_test, y_pred)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=' Random Forest(area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Courbe ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "rf= LogisticRegression()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "param_grid={'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],'max_iter':[10, 100,500,1000, 2000, 3000,4000]}\n",
    "grid2=GridSearchCV( LogisticRegression(),param_grid, cv=StratifiedKFold(5)  )\n",
    "grid2.fit(X1_train,y1_train)\n",
    "print('best score',grid2.best_score_)\n",
    "print('best score',grid2.best_params_)\n",
    "model2=grid2.best_estimator_\n",
    "print('best score',model2.score(MX_test,My_test))\n",
    "N, train_score, val_score=learning_curve(model2, X1_train, y1_train, train_sizes=np.linspace(0.2, 1.0,5), cv=StratifiedKFold(5)  )\n",
    "plt.plot(N,val_score.mean(axis=1), label= 'validation')\n",
    "plt.plot(N,train_score.mean(axis=1), label= 'train')\n",
    "plt.xlabel('train_sizes')\n",
    "plt.legend()\n",
    "print(confusion_matrix(My_test, model2.predict(MX_test)))  \n",
    "print(classification_report(My_test,model2.predict(MX_test)))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "y_pred=model2.predict_proba(MX_test)[:,1]\n",
    "logit_roc_auc = roc_auc_score(My_test,y_pred)\n",
    "print(logit_roc_auc)\n",
    "fpr, tpr, thresholds = roc_curve(My_test, y_pred)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='LogisticRegression(area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Courbe ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVES BAYES ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB  \n",
    "param_grid={'var_smoothing': np.linspace(0.1,1,10)}\n",
    "grid3=GridSearchCV( GaussianNB(),param_grid, cv=StratifiedKFold(5)  )\n",
    "grid3.fit(X1_train,y1_train)\n",
    "\n",
    "\n",
    "print('best score',grid3.best_score_)\n",
    "print('best score',grid3.best_params_)\n",
    "model3=grid3.best_estimator_\n",
    "print('best score',model3.score(MX_test,My_test))\n",
    "N, train_score, val_score=learning_curve(model3, X1_train, y1_train, train_sizes=np.linspace(0.2, 1.0,5), cv=cv2)\n",
    "\n",
    "plt.plot(N,val_score.mean(axis=1), label= 'validation')\n",
    "plt.plot(N,train_score.mean(axis=1), label= 'train')\n",
    "plt.xlabel('train_sizes')\n",
    "plt.legend()\n",
    "\n",
    "print(confusion_matrix(My_test, model3.predict(MX_test)))  \n",
    "print(classification_report(My_test,model3.predict(MX_test)))\n",
    "\n",
    "\n",
    "y_pred=model3.predict_proba(MX_test)[:,1]\n",
    "logit_roc_auc = roc_auc_score(My_test,y_pred)\n",
    "print(logit_roc_auc)\n",
    "fpr, tpr, thresholds = roc_curve(My_test, y_pred)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=' Naive Bays(area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Courbe ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM avec kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC  \n",
    "param_grid={'kernel':['rbf','linear','sigmoid']}\n",
    "grid4=GridSearchCV(SVC(probability=True),param_grid, cv=cv2)\n",
    "grid4.fit(X1_train,y1_train)\n",
    "print('best score',grid4.best_score_)\n",
    "print('best score',grid4.best_params_)\n",
    "model4=grid4.best_estimator_\n",
    "print('best score',model4.score(MX_test,My_test))\n",
    "N, train_score, val_score=learning_curve(model4, X1_train, y1_train, train_sizes=np.linspace(0.2, 1.0,5), cv=cv2)\n",
    "\n",
    "plt.plot(N,val_score.mean(axis=1), label= 'validation')\n",
    "plt.plot(N,train_score.mean(axis=1), label= 'train')\n",
    "plt.xlabel('train_sizes')\n",
    "plt.legend()\n",
    "\n",
    "# print(confusion_matrix(My_test, model4.predict(MX_test)))  \n",
    "print(classification_report(My_test,model4.predict(MX_test)))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "y_pred=model4.predict_proba(MX_test)[:,1]\n",
    "logit_roc_auc = roc_auc_score(My_test,y_pred)\n",
    "print(logit_roc_auc)\n",
    "fpr, tpr, thresholds = roc_curve(My_test, y_pred)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Artifieal Neural Network(area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Courbe ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "random.seed(1000)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X1_train)\n",
    "X1_train = scaler.transform(X1_train)\n",
    "X1_test = scaler.transform(MX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={'max_iter':[10,50,100, 200, 300,400]}\n",
    "grid5=GridSearchCV(MLPClassifier(hidden_layer_sizes=(12,12,12)),param_grid, cv=cv2)\n",
    "grid5.fit(X1_train,y1_train)\n",
    "print('best score',grid5.best_score_)\n",
    "print('best score',grid5.best_params_)\n",
    "model5=grid5.best_estimator_\n",
    "print('best score',model5.score(X1_test,My_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, train_score, val_score=learning_curve(model5, X1_train, y1_train, train_sizes=np.linspace(0.2, 1.0,5), cv=cv2)\n",
    "\n",
    "plt.plot(N,val_score.mean(axis=1), label= 'validation')\n",
    "plt.plot(N,train_score.mean(axis=1), label= 'train')\n",
    "plt.xlabel('train_sizes')\n",
    "plt.legend()\n",
    "\n",
    "print(confusion_matrix(My_test, model5.predict(X1_test)))  \n",
    "print(classification_report(My_test,model5.predict(X1_test)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "y_pred=model5.predict_proba(X1_test)[:,1]\n",
    "logit_roc_auc = roc_auc_score(My_test,y_pred)\n",
    "print(logit_roc_auc)\n",
    "fpr, tpr, thresholds = roc_curve(My_test, y_pred)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Artifieal Neural Network(area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Courbe ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
